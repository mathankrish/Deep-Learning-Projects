{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF_5oOZ9OEaG"
   },
   "source": [
    "Welcome to the first handson!!!\n",
    "- In this handson you will be building an deep neural network network by integrating batch normalization\n",
    "- You will also be implementing minibatch gradient and L2 regularization to train you network\n",
    "- Follow the instruction provided for cell to write the code in each cell.\n",
    "- Run the below cell for to import necessary packages to read and visualize data\n",
    "- Before submit your notebook. Restart the kernel and run all the cell. Make sure that any cell shouldn't cause any error or problem.\n",
    "- Don't forget to run the last cell in the jupyter notebook, failing which your efforts will be invalid.\n",
    "- Don't delete any cell given in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yaDtyHKHVAKL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from test_opthyptuning_batchnorm import batchnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivY_tF8yVKex"
   },
   "source": [
    "The data is provided as file named 'data.csv'.  \n",
    "Using pandas read the csv file and assign the resulting dataframe to variable 'data'   \n",
    "for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0BHA1dAKyzw8"
   },
   "outputs": [],
   "source": [
    "###Start code here\n",
    "data =\n",
    "###End code here\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqCtS1D3Wi44"
   },
   "source": [
    "- Extract feature1 and feature2 values from dataframe 'df' and assign it to variable 'X'\n",
    "- Extract target variable 'traget' and assign it to variable 'y'.  \n",
    "Hint:\n",
    " - Use .values to exract values from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IaRJ8HLLXpuK"
   },
   "outputs": [],
   "source": [
    "###Start code here\n",
    "X = \n",
    "y =\n",
    "###End code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n",
    "- The green spots corresponds to target value 0 and blue spots corresponds to target value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OLSGMjm0VGkJ"
   },
   "outputs": [],
   "source": [
    "colors=['green','blue']\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "#Plot the figure\n",
    "plt.figure()\n",
    "plt.title('Non-linearly separable classes')\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n",
    "            s=25, edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n",
    "- Transpose X and assign it to variable 'X_data'\n",
    "- reshape y to have shape (1, number of samples) and assign to variable 'y_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XchlAZPwHpMk"
   },
   "outputs": [],
   "source": [
    "###Start code here\n",
    "X_data = \n",
    "y_data = \n",
    "###End code here\n",
    "\n",
    "assert X_data.shape == (2, 1000)\n",
    "assert y_data.shape == (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network dimension to have **two** input features, **four** **hidden layers** with **20** nodes each, one output node at final layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t_dEEaJykLSa"
   },
   "outputs": [],
   "source": [
    "###Start code here\n",
    "layer_dims =\n",
    "###End code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  #import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n",
    "- Set the datatype of placeholders as **float32**\n",
    "- parameters - num_features\n",
    "- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4kp4_9kS58CT"
   },
   "outputs": [],
   "source": [
    "def placeholders(num_features):\n",
    "    ###Start code here\n",
    "    A_0 = \n",
    "    Y = \n",
    "    ###End code\n",
    "    return A_0,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function named initialize_parameters_deep() to initialize weights and bias for each layer\n",
    "- Use tf.get_variable to initialise weights and bias, set datatype as **float32**\n",
    "- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n",
    "- Parameters - layer_dims\n",
    "- Returns - dictionary of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W3l_vyXVkrlw"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    tf.set_random_seed(1)\n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        ###Start code here\n",
    "        parameters['W' + str(l)] = \n",
    "        parameters['b' + str(l)] = \n",
    "        ###End code\n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functon named linear_forward_prop() to define forward propagation for a given layer.\n",
    "- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n",
    "- returns: A(output from the current layer)\n",
    "- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid\n",
    "- After computing linear output Z implement batch normalization before feeding to activation function, **set traing = True and axis = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SKbSUt5g4toV"
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(A_prev,W,b, activation):\n",
    "    ###Start code here\n",
    "    Z =                              #call linear_fowrward prop\n",
    "    Z =                              #implement batch normalization on Z\n",
    "    ###End code\n",
    "    if activation == \"sigmoid\":\n",
    "        A = \n",
    "    elif activation == \"relu\":\n",
    "        A = \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward propagation for entire network as l_layer_forward()\n",
    "- Parameters: A_0(input data), parameters(dictionary of weights and bias)\n",
    "- returns: A(output from final layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Cdks1w__k-Ei"
   },
   "outputs": [],
   "source": [
    "def l_layer_forwardProp(A_0, parameters):\n",
    "    A = A_0\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "    ###Start code\n",
    "        A =                  #call linear forward prop with relu activation\n",
    "    A =                      #call linear forward prop with sigmoid activation\n",
    "    ###End code\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the cost function\n",
    "- parameters:\n",
    "  - Z_final: output fro final layer\n",
    "  - Y: actual output\n",
    "  - parameters: dictionary of weigths and bias\n",
    "  - regularization : boolean\n",
    "  - lambd: regularization parameter\n",
    "- First define the original cost using tensoflow's sigmoid_cross_entropy function\n",
    "- If **regularization == True** add regularization term to original cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cost(Z_final, Y , parameters, regularization = False, lambd = 0):\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y)\n",
    "    if regularization:\n",
    "        reg_term = 0\n",
    "        L = len(parameters)//2\n",
    "        for l in range(1,L+1):\n",
    "            ###Start code\n",
    "            reg_term +=                #add L2 loss term\n",
    "            ###End code\n",
    "        cost = cost + (lambd/2) * reg_term\n",
    "    return tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to generate mini-batches. **Important: Use np.random.permutation for generate random indicies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_samples_minibatch(X, Y, batch_size, seed = 1):\n",
    "    np.random.seed(seed)\n",
    "    ###Start code\n",
    "    m =                                            #number of samples\n",
    "    num_batches =                                 #number of batches derived from batch_size\n",
    "    ###End code\n",
    "    indices =                                   # generate ramdom indicies, use np.random.permutation\n",
    "    shuffle_X = X[:,indices]\n",
    "    shuffle_Y = Y[:,indices]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #generate minibatch\n",
    "    for i in range(num_batches):\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        \n",
    "        assert X_batch.shape == (X.shape[0], batch_size)\n",
    "        assert Y_batch.shape == (Y.shape[0], batch_size)\n",
    "        \n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    \n",
    "    #generate batch with remaining number of samples\n",
    "    if m % batch_size != 0:\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model to train the network using minibatch\n",
    "- parameters:\n",
    "  - X_train, Y_train: input and target data\n",
    "  - layer_dims: network configuration\n",
    "  - learning_rate\n",
    "  - num_iter: number of epoches\n",
    "  - mini_batch_size: number of samples to be considered in each minibatch\n",
    "- return: dictionary of trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_aBIc5RIRz-q"
   },
   "outputs": [],
   "source": [
    "def model_with_minibatch(X_train,Y_train, layer_dims, learning_rate,num_iter, mini_batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    num_features, num_samples = X_train.shape\n",
    "    ###Start code\n",
    "    A_0, Y =                         #call placeholder function to initialize placeholders A_0 and Y\n",
    "    parameters =                    #Initialse Weights and bias using initialize_parameters\n",
    "    Z_final =                       #call the function l_layer_forwardProp() to define the final output\n",
    "    \n",
    "    cost =                          #call the final_cost function with regularization set TRUE\n",
    "    ###End code\n",
    "    \n",
    "    #use adam optimization to train the network\n",
    "    train_net = \n",
    "    seed = 1\n",
    "    num_minibatches = int(num_samples / mini_batch_size)\n",
    "    init = tf.global_variables_initializer()\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_iter):\n",
    "            epoch_cost = 0\n",
    "            ###Start code\n",
    "            mini_batches =               #call random_sample_minibatch to return minibatches\n",
    "            ###End code\n",
    "            seed = seed + 1\n",
    "            \n",
    "            #perform gradient descent for each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                ###Start code\n",
    "                X_batch, Y_batch =             #assign minibatch\n",
    "                ###End code\n",
    "                _,mini_batch_cost = sess.run([train_net, cost], feed_dict={A_0: X_batch, Y: Y_batch})\n",
    "                \n",
    "                epoch_cost += mini_batch_cost/num_minibatches\n",
    "            if epoch % 2 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "            if epoch % 100 == 0:\n",
    "                print(epoch_cost)\n",
    "        batchnorm.save_ans7(epoch_cost)\n",
    "        plt.ylim(0 ,2, 0.0001)\n",
    "        plt.xlabel(\"epoches per 2\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        params = sess.run(parameters)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model using the above defined function\n",
    "- Use X_data and y_data as training input, learning rate = 0.001, numiteration = 1000  \n",
    "  minibatch size = 256\n",
    "- Return the trained parameters to variable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Start code\n",
    "parameters =\n",
    "###End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cells to save your answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchnorm.save_func1(placeholders)\n",
    "batchnorm.save_func2(initialize_parameters_deep)\n",
    "batchnorm.save_func3(linear_forward_prop)\n",
    "batchnorm.save_func4(l_layer_forwardProp)\n",
    "batchnorm.save_func5(final_cost)\n",
    "batchnorm.save_func6(random_samples_minibatch)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DNN_hands_on_solution.ipynb",
   "provenance": [
    {
     "file_id": "1i6MJxD7kd04W6ojetXVPsNUeA4qGg6Zq",
     "timestamp": 1525158878710
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
