{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to you first hands-on on word embeddings!!!\n",
    "\n",
    "- In this hands-on you will be using pretrained word vectors from stanford nlp which you can find [here](https://nlp.stanford.edu/projects/glove/)\n",
    "- Follow the instruction provided for cell to write the code in each cell.\n",
    "- Before submit your notebook. Restart the kernel and run all the cell. Make sure that any cell shouldn't cause any error or problem.\n",
    "- Don't forget to run the last cell in the jupyter notebook, failing which your efforts will be invalid.\n",
    "- Don't delete any cell given in the notebook.\n",
    "\n",
    "#### Each word vectors is of dimension 50\n",
    "#### You will be performing following operations:\n",
    "    - Load the pretrained vectors from the text file\n",
    "    - Write a function to find cosine similarity between two word vectors\n",
    "    - Write an function to find analogy analogy problems such as King : Queen :: Men : __?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1\n",
    "- A text file having the trained word vectors is provided for you as word2vec.txt in the same working directory.\n",
    "- Each line in the file is space seperated values where first value is the word and the remaing values are its vector representation.\n",
    "\n",
    "### Define a function get_word_vectors()\n",
    "    parameters: file_name  \n",
    "    returns: word_to_vec: dictionary with key as the word and the value is the corresponding word vectors as 1-d array each element of type float32.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_word_vectors(file_name):\n",
    "    ###Start code here\n",
    "    with open(file_name, 'r',encoding =\"utf8\") as f:\n",
    "        \n",
    "        word_to_vec = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "#             words.add(curr_word)\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###End code\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the function you defined above read the word vectors from the file word_vectors.txt and assign it to variable word_to_vec\n",
    "\n",
    "### Expected output  (showing only first few values of vectors)\n",
    "   Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071 ...]  \n",
    "   mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767 ....]  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071\n",
      " -1.0695     0.76847   -0.87813   -0.0080954  0.43884    1.0476\n",
      " -0.45071   -0.58931    0.83246   -0.038442  -0.73533    0.26389\n",
      "  0.12617    0.57623   -0.23866    1.0922    -0.3367     0.081537\n",
      "  0.84798   -2.4795    -0.40351   -0.84087    0.12034    0.29074\n",
      "  1.9711    -0.50886   -0.45977   -0.13617    0.55613    0.22924\n",
      " -0.18947    0.43544    0.65151    0.043537  -0.1162     0.72196\n",
      " -0.66163   -0.17272    0.27367   -0.28169   -0.82025   -1.5089\n",
      "  0.052787  -0.035579 ]\n",
      "mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767\n",
      " -0.93533    0.76088   -0.0056654 -0.063649   0.30297    0.52401\n",
      "  0.2843    -0.38162    0.98797    0.093184  -1.1464     0.070523\n",
      "  0.58012    0.50644   -0.24026    1.7344     0.020735   0.43704\n",
      "  1.2148    -2.2483    -0.41168   -0.24922    0.31225   -0.49464\n",
      "  2.0441    -0.012111  -0.19556    0.085665   0.27682    0.015702\n",
      "  0.0067683  0.12759    0.87008   -0.40641   -0.21057    0.41651\n",
      " -0.021812  -0.53649    0.54095   -0.43442   -0.52489   -2.0277\n",
      "  0.13136    0.11704  ]\n"
     ]
    }
   ],
   "source": [
    "word_to_vec = get_word_vectors('word2vec.txt')\n",
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "print(\"Father: \", father)\n",
    "print(\"mother: \", mother)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2 Determine the cosine similarity between two word vectors\n",
    "- The formula for cosine similarity is given by\n",
    "  score = $\\large \\frac{U.V}{\\sqrt{||U||.||V||}}$ where ||U|| and ||V|| is the sum of the squares of the elemnts individual vectors\n",
    "  \n",
    "\n",
    "### Define a function named cosine_similarity()\n",
    "    - parameters u, v are the word vectors whose similarity has to be determined\n",
    "    - returns - score: cosine similarity of u and v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    ###Start code here\n",
    "    distance = 0.0\n",
    "    N = np.dot(u,v)\n",
    "    D = np.linalg.norm(u,2)*np.linalg.norm(v,2)\n",
    "    score = N/D\n",
    "    \n",
    "    ###End code\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the bellow cell to find the similarity between word vectors paris and rome\n",
    "### Expected output\n",
    "   similarity score : 0.7099411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity score : 0.7099411341712597\n"
     ]
    }
   ],
   "source": [
    "paris = word_to_vec[\"paris\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "print(\"similarity score :\", cosine_similarity(paris, rome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "In the word analogy task, we complete the analogy . In detail, we are trying to find a word d, such that the associated word vectors $u_1, v_1, u_2, v_2$ are related in the following manner: $u_1 - v_1 \\approx u_2 - v_2$. We will measure the similarity between $u_1 - v_1$ and $u_2 - v_2$ using cosine similarity.\n",
    "#### As an example,  to find the best possible word for the analogy King : Queen :: Men : __?_ you will perform following steps:\n",
    "- extract word vectors of three words king, queen and men\n",
    "- find the element wise difference between the two word vectors king and queen as V1\n",
    "- Find the element wise difference between the word vector men and each word vector in word_to_vec ditionary as V2 (while doing so exclude the words of interest ie. king, queen and men)\n",
    "- Find the cosine similarity between vector V1 and V2 and choose the word from the word_to_vec ditionary that has maximum similarity between V1 and V2.\n",
    "### Define the function named find_analogy()\n",
    "    - parameters: word1 - string corresponding to word vector $u_1$, word2 - string corresponding to word vector $v_1$, word3 - string corresponding to word vector $u_2$, word_to_vec - dictionary of words and their corresponding vectors\n",
    "    - returns: best_word -  the word such that $u_1$ - $v_1$ is close to $v\\_best\\_word$ - $v_c$, as measured by cosine similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word_1, word_2, word_3, word_to_vec):\n",
    "    ####Start code here\n",
    "    word_1 = word_1.lower(); \n",
    "    word_2 = word_2.lower();\n",
    "    word_3 = word_3.lower();\n",
    "    \n",
    "    # finding word embeddings  of word a ,b ,c\n",
    "    e_a = word_to_vec[word_1]\n",
    "    e_b = word_to_vec[word_2]\n",
    "    e_c = word_to_vec[word_3]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_1, word_2, word_3] :\n",
    "            continue\n",
    "        \n",
    "        \n",
    "       # e_b - e_a â‰ˆ w - e_c\n",
    "        cosine_sim = cosine_similarity(e_b-e_a, word_to_vec[w]-e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim \n",
    "        # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###End code\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below  code to check your above defined function\n",
    "\n",
    "#### Expected output:\n",
    "    father -> son :: mother -> daughter\n",
    "    india -> delhi :: japan -> tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father -> son :: mother -> daughter\n",
      "india -> delhi :: japan -> tokyo\n"
     ]
    }
   ],
   "source": [
    "print ('{} -> {} :: {} -> {}'.format('father', 'son', 'mother',find_analogy('father', 'son', 'mother', word_to_vec)))\n",
    "print ('{} -> {} :: {} -> {}'.format('india', 'delhi', 'japan',find_analogy('india', 'delhi', 'japan', word_to_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cells to save your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nlpusingdp_word2vec import word2vec as w2v\n",
    "w2v.save_ans1(find_analogy, word_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
