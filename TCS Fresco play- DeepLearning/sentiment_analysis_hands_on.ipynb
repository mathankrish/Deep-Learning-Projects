{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the handson!!!\n",
    "- In this hands-on you will be building a sentiment classifier on for movie reviews using word vectors and LSTM.\n",
    "- Follow the instruction provided for cell to write the code in each cell.\n",
    "- Run the below cell for to import necessary packages to read and visualize data\n",
    "- Before submit your notebook. Restart the kernel and run all the cell. Make sure that any cell shouldn't cause any error or problem.\n",
    "- Don't forget to run the last cell in the jupyter notebook, failing which your efforts will be invalid.\n",
    "- Don't delete any cell given in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary packages in the below cell as and when you require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.datasets.imdb import get_word_index\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset.\n",
    "- Keras has a built in function to download movie review available in imdb. \n",
    "- Each words in the review are represented by their unique index and the labels are in binary format representing positive or negative reviews\n",
    "- The necessary code to download the dataset has been written for you.\n",
    "- The variable **word_to_id** is a dictionary containing words and their corresponding ids\n",
    "- Run the below cell to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "word to id fist five samples {\"ferdie's\": 51429, 'ojibway': 28690, 'saizescus': 58403, 'grazing': 31610, 'unfaltering': 65637}\n",
      "\n",
      "\n",
      "sample input\n",
      " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "\n",
      "\n",
      "target output 1\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=vocab_size)\n",
    "word_to_id = get_word_index()\n",
    "print(\"word to id fist five samples {}\".format({key:value for key, value in zip(list(word_to_id.keys())[:5], list(word_to_id.values())[:5])}))\n",
    "print(\"\\n\")\n",
    "print(\"sample input\\n\", X_train[0])\n",
    "print('\\n')\n",
    "print(\"target output\", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each review in the dataset has some special tokens such as\n",
    "    - <START> : to identify the start of the sentence\n",
    "    - <UNK> : If some words are not identified in the vocabulary\n",
    "    - <PAD> : The value to be filled if sequence requires padding\n",
    "### Task 1:\n",
    "    - offset the word_to_id dictionary by three values such that 0,1,2 represents START, UNK, PAD respectively\n",
    "    - Once you perform the above step reverse the word_to_id dictionary to represent ids as keys and words as values. Assign the resulting dictionary to id_to_word variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_to_id[\"PAD\"] = 0\n",
    "word_to_id[\"START\"] =1 \n",
    "word_to_id[\"UNK\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below code to view the first review in training samples\n",
    "\n",
    "### Expected output\n",
    "START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNK is an amazing actor and now the same being director UNK father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for UNK and would recommend it to everyone to watch and the fly UNK was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also UNK to the two little UNK that played the UNK of norman and paul they were just brilliant children are often left out of the UNK list i think because the stars that play them all grown up are such a big UNK for the whole film but these children are amazing and should be UNK for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was UNK with us all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room UNK it so heart shows to years of every never going UNK help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but UNK to story wonderful that in seeing in character to of 70s UNK with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other UNK in of seen over UNK for anyone of UNK br show's to whether from than out themselves history he name half some br of UNK odd was two most of mean for 1 any an boat she he should is thought UNK but of script you not while history he heart to real at UNK but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with UNK film want an\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([id_to_word[i] for i in X_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since each movie reviews are of variable lengths in terms of number of words, so it is necessay to fix the review lenght to few words say upto first 500 words.\n",
    "### Task 3\n",
    "   - For each of the samples of X_train and X_test sample upto first 500 words\n",
    "   - If reviews are less than 500 words pad the sequence with zeros in the beginning to make up the length upto 500\n",
    "   - Assign the padded sequence to X_train_pad and X_test_pad variables for train and test smaples respectively\n",
    "   \n",
    "   [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
    "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
    "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
    "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
    "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
    "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
    "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
    "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
    "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
    "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
    "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
    "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
    "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
    "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
    "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
    " 4472  113  103   32   15   16    2   19  178   32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16    2   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "X_train_pad = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test_pad =  keras.preprocessing.sequence.pad_sequences(X_test, maxlen=500)\n",
    "print(X_train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras [Sequential](https://keras.io/getting-started/sequential-model-guide/) to build an LSTM model using the below specifications\n",
    "- Add an embedding layer( the look up table) such that vacabulary size is 5000 and each word in the vocabulary is 32 dimension vector\n",
    "- Add an LSTM layer with 100 hidden nodes\n",
    "- Add a final sigmoid activation layer\n",
    "- Use adam optimizer and binary cross entropy loss, and metrics as accuracy\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "#### Layer (type)                 Output Shape              Param #   \n",
    "=================================================================  \n",
    "embedding_5 (Embedding)      (None, None, 32)          160000      \n",
    "_________________________________________________________________  \n",
    "lstm_5 (LSTM)                (None, 100)               53200     \n",
    "_________________________________________________________________  \n",
    "#### dense_5 (Dense)              (None, 1)                 101       \n",
    "=================================================================    \n",
    "Total params: 213,301   \n",
    "Trainable params: 213,301   \n",
    "Non-trainable params: 0  \n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32 \n",
    "model = keras.models.Sequential() \n",
    "model.add(Embedding(input_dim=5000, output_dim=embedding_vector_length))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "# model.add(keras.layers.Dense(units=256, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with X_train_pad and Y_train as train data and X_test_pad, Y_test as Validation set\n",
    "    - set the number of epochs to 3\n",
    "    - set batch size to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 590s 2s/step - loss: 0.4609 - accuracy: 0.7726 - val_accuracy: 0.8553 - val_loss: 0.3461\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 577s 1s/step - loss: 0.3034 - accuracy: 0.8784 - val_accuracy: 0.8660 - val_loss: 0.3242\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 576s 1s/step - loss: 0.2477 - accuracy: 0.9040 - val_accuracy: 0.8606 - val_loss: 0.3273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad7dc89b00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Start code here\n",
    "model.fit(X_train_pad, Y_train, batch_size = 64, validation_data = (X_test_pad, Y_test), epochs = 3)\n",
    "\n",
    "###End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to run the model prediction on custom samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked the movie and had fun. Sentiment: 0.5536186\n",
      "this movie was terrible and bad. Sentiment: 0.54603076\n",
      "32/32 [==============================] - 4s 125ms/step - loss: 0.3394 - accuracy: 0.8540\n"
     ]
    }
   ],
   "source": [
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = keras.preprocessing.sequence.pad_sequences([tmp], maxlen=500) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(np.array([tmp_padded][0]))[0][0]))\n",
    "\n",
    "accuracy = model.evaluate(X_test_pad[:1000], Y_test[:1000])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cells to save your answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nlpusingdp_sentimentanalysis import sentimentclassification\n",
    "sentimentclassification.save_ans1(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
