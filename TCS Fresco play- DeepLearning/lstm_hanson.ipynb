{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this hands on you will build a model that once trained on a peice of text data can generate its own sequnce of words in a similar fashion as in trained data.\n",
    "\n",
    "- Follow the instructions provided for each cell and and code accordingly. \n",
    "- In order to run the cell press shift+enter.\n",
    "- make sure you have run all the cells before submitting the hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text data from story.txt file and split the text into seperate tokens, assign thr array of tokens to variable *training_data*\n",
    "\n",
    "### Expected output:\n",
    "    array(['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general',\n",
    "       'council', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general', 'council', 'to']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Start code here\n",
    "\n",
    "\n",
    "\n",
    "training_data = open(\"story.txt\").read().split(\" \")\n",
    "###End code\n",
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the unique tokens in training_data nas sort them alphabetical order and assign the sorted list to variable words\n",
    "### create dictionary ind_to_word to map index to word\n",
    "### create another dictionary word_to_ind to reverse map word to their respective index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['', ',', '.', '.\\n', '?', 'a', 'about', 'ago', 'agree', 'all'] \n",
      "\n",
      "index_to_words:  [(0, ''), (1, ','), (2, '.'), (3, '.\\n'), (4, '?'), (5, 'a'), (6, 'about'), (7, 'ago'), (8, 'agree'), (9, 'all')] \n",
      "\n",
      "word_to_index:  [('', 0), ('remedies', 73), ('approach', 15), ('?', 4), ('until', 96), ('neighbourhood', 61), ('long', 50), ('to', 94), ('is', 46), ('of', 64)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Start code here\n",
    "words = sorted(set(training_data))\n",
    "ind_to_word = {i:w for i, w in enumerate(words)}\n",
    "word_to_ind = {w:i for i, w in enumerate(words)}\n",
    "###End code\n",
    "print(\"words: \", words[:10], \"\\n\")\n",
    "print(\"index_to_words: \", list(ind_to_word.items())[:10], \"\\n\")\n",
    "print(\"word_to_index: \", list(word_to_ind.items())[:10], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to generate training dataset \n",
    "    - parameters: dataset: orginal dataset\n",
    "                  look_back: the window size that tells the number of previous values in the series to look for to                   predict the next one.\n",
    "    - returns: feature and target arrays\n",
    "    \n",
    "example: \n",
    "         for window size 1:\n",
    "         dataset = [1,2,3,4]  \n",
    "         feature = [[1],[2],[3]]    \n",
    "         target = [2,3,4]  \n",
    "         \n",
    "         for window size 2:\n",
    "         feature = [[1,2],[2,3]]  \n",
    "         target = [3,4]  \n",
    "### Expected output when you when you call generate_dataset on training_data and look_back = 10 :\n",
    "input:  [[48, 5, 0, 85, 56, 37, 3, 35, 28, 92], [5, 0, 85, 56, 37, 3, 35, 28, 92, 25], [0, 85, 56, 37, 3, 35, 28, 92, 25, 102]]  \n",
    "labels:  [25, 102, 53]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[50, 7, 1, 87, 58, 39, 5, 37, 30, 94], [7, 1, 87, 58, 39, 5, 37, 30, 94, 27], [1, 87, 58, 39, 5, 37, 30, 94, 27, 104]]\n",
      "labels:  [27, 104, 55]\n"
     ]
    }
   ],
   "source": [
    "####Start code here\n",
    "def generate_dataset(dataset, look_back=10):\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back - 1):\n",
    "       \n",
    "        seq_in = dataset[i:i + look_back]\n",
    "        #print('seq in : ', seq_in)\n",
    "        seq_out = dataset[i + look_back]\n",
    "        #print('seq out : ', seq_out)\n",
    "        dataX.append([word_to_ind[char] for char in seq_in])\n",
    "        dataY.append(word_to_ind[seq_out])\n",
    "    return dataX, dataY\n",
    "\n",
    "    \n",
    "###End code\n",
    "\n",
    "inputs, labels = generate_dataset(training_data, 10)\n",
    "print(\"input: \", inputs[:3])\n",
    "print(\"labels: \", labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is to  reshape the inputs and normalize them.\n",
    "    - This step is coded for you\n",
    "    - Run the below cell to prepare the data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 10\n",
    "X_modified = np.reshape(inputs, (len(inputs), look_back, 1))\n",
    "X_modified = X_modified / float(len(words))\n",
    "Y_modified = np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras Sequential() class create a model having two LSTM block and one fully connected  layer with activation softmax\n",
    "### apply dropout with probability p = 0.2 between the layers of LSTM\n",
    "### compile the model with categorical_crossentropy loss and  adam optimizer\n",
    "\n",
    "### Expected output\n",
    "<img src=\"lstm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 114)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 10, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 114)               45714     \n",
      "=================================================================\n",
      "Total params: 1,970,514\n",
      "Trainable params: 1,970,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(51)\n",
    "###Start code here\n",
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "###End code\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model.fit() on train data with features as X_modified and target as Y_modified for 50 epoches and batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "194/194 [==============================] - 5s 28ms/step - loss: 4.7290\n",
      "Epoch 2/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.5844\n",
      "Epoch 3/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.5346\n",
      "Epoch 4/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4851\n",
      "Epoch 5/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4801\n",
      "Epoch 6/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4514\n",
      "Epoch 7/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4505\n",
      "Epoch 8/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4639\n",
      "Epoch 9/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4636\n",
      "Epoch 10/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4328\n",
      "Epoch 11/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4291\n",
      "Epoch 12/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4422\n",
      "Epoch 13/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4529\n",
      "Epoch 14/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4305\n",
      "Epoch 15/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.4206\n",
      "Epoch 16/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.3469\n",
      "Epoch 17/50\n",
      "194/194 [==============================] - 4s 22ms/step - loss: 4.2963\n",
      "Epoch 18/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.2731\n",
      "Epoch 19/50\n",
      "194/194 [==============================] - 4s 21ms/step - loss: 4.2006\n",
      "Epoch 20/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.2039\n",
      "Epoch 21/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 4.0872\n",
      "Epoch 22/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.9841\n",
      "Epoch 23/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.9013\n",
      "Epoch 24/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.8194\n",
      "Epoch 25/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.7044\n",
      "Epoch 26/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.5626\n",
      "Epoch 27/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.5056\n",
      "Epoch 28/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.3690\n",
      "Epoch 29/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.1976\n",
      "Epoch 30/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.2100\n",
      "Epoch 31/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 3.1986\n",
      "Epoch 32/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.9182\n",
      "Epoch 33/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.8770\n",
      "Epoch 34/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.6296\n",
      "Epoch 35/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.5004\n",
      "Epoch 36/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.5087\n",
      "Epoch 37/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.4565\n",
      "Epoch 38/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 2.1427\n",
      "Epoch 39/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.9647: 0s - loss: 1.93\n",
      "Epoch 40/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.9343\n",
      "Epoch 41/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.7662\n",
      "Epoch 42/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.8178\n",
      "Epoch 43/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.7806\n",
      "Epoch 44/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.6851\n",
      "Epoch 45/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.4913\n",
      "Epoch 46/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.3164\n",
      "Epoch 47/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.2800\n",
      "Epoch 48/50\n",
      "194/194 [==============================] - 4s 21ms/step - loss: 1.2081\n",
      "Epoch 49/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.1337\n",
      "Epoch 50/50\n",
      "194/194 [==============================] - 4s 20ms/step - loss: 1.2917\n"
     ]
    }
   ],
   "source": [
    "###Start code here\n",
    "train_logs = model.fit(X_modified, Y_modified, epochs = 50, batch_size = 10) \n",
    "###End code\n",
    "with open(\"output.txt\", \"w+\") as file:\n",
    "    file.write(\"train score {0:.2f}\\n\".format(train_logs.history[\"loss\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below codes takes a random sequence of words and generates more sequnce using the model you trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " which he thought would meet the case . you will met agree some up bell to this easily , case an old we we our up danger cat . this propose met with said always know had mice propose means we we up is had you their the agree met with , cat . agree , should very , this danger the the cat it the case bell all remedies looked should very , , of it but and we always this danger what the the would small outwit easy will cat proposal met met with , and , looked remedies got we always know danger the  but make\n"
     ]
    }
   ],
   "source": [
    "string_mapped = inputs[50].copy()\n",
    "full_string = [ind_to_word[value] for value in string_mapped]\n",
    "# generating characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(words))\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "    seq = [ind_to_word[value] for value in string_mapped]\n",
    "    full_string.append(ind_to_word[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n",
    "\n",
    "    \n",
    "txt=\"\"\n",
    "for word in full_string:\n",
    "    txt = txt+\" \"+word\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
